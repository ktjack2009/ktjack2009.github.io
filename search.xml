<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[tf-idf权重计算]]></title>
    <url>%2F2019%2F05%2F06%2Ftf-idf%E6%9D%83%E9%87%8D%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[tf-idf中文称为词频-逆文档频率，用以计算词项对于一个文档集或一个语料库中的一份文件的重要程度。 原理词项的重要性随着它在文档中出现的次数成正比增加，但同时会随着它在文档集中出现的频率成反比下降。 概念tf(term frequency)代表词项频率。词频标准化： tf = 单词在文档中出现的次数 / 文档的总词数 tf = √单词在文档中出现的次数 df(document frequency)代表文档频率逆文档频率： idf = log(文档集的总文档数 / (包含某个词的文档数 + 1)) tf-idf = tf * idf 通过tf-idf可以把文档表示成n维的词项权重向量。]]></content>
      <categories>
        <category>学习记录</category>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>tf-idf</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[星星日记01]]></title>
    <url>%2F2018%2F10%2F31%2F%E6%98%9F%E6%98%9F%E6%97%A5%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[星星黄疸有点高，需要晒晒太阳，星星要健健康康的长大哟！]]></content>
      <categories>
        <category>星星日记</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>成长</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[星星日记00]]></title>
    <url>%2F2018%2F10%2F29%2F%E6%98%9F%E6%98%9F%E6%97%A5%E8%AE%B000%2F</url>
    <content type="text"><![CDATA[2018年10月29日19点56分。由于爸爸昨天和妈妈散步的时候和星星说，星星要快点出来，结果星星今天迫不及待的就跑出来了，妈妈都还没来得及准备。好在星星比较乖，没有让妈妈太疼，顺顺利利的跑出来了，女儿棒棒的!【ps：由于爸爸笨，才知道有这种方式记录星星的成长，希望星星长大了之后不要怪爸爸^_^】]]></content>
      <categories>
        <category>星星日记</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>成长</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[星星日记【pre】]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%98%9F%E6%98%9F%E6%97%A5%E8%AE%B0%E3%80%90pre%E3%80%91%2F</url>
    <content type="text"><![CDATA[星星还有2个月就要出来和爸爸妈妈见面了…]]></content>
      <categories>
        <category>星星日记</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>成长</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch集群入门]]></title>
    <url>%2F2018%2F05%2F07%2FElasticsearch%E9%9B%86%E7%BE%A4%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[索引管理使用python的requests包来实现12&gt;&gt;&gt; requests.put(&apos;http://localhost:9200/blog&apos;).text&gt;&gt;&gt; &#123;&quot;acknowledged&quot;:true,&quot;shards_acknowledged&quot;:true,&quot;index&quot;:&quot;blog&quot;&#125; acknowledged的值为true，说明新建索引成功【索引名称不能有大写字母】。如果索引已经存在，新建同名索引会报错。 自定义分片数和副本数的索引12&gt;&gt;&gt; data = &#123;&quot;settings&quot;: &#123;&quot;number_of_shards&quot;: 3, &quot;number_of_replicas&quot;: 0&#125;&#125;&gt;&gt;&gt; requests.put(&apos;http://localhost:9200/blog&apos;, json=data).text 更新副本12&gt;&gt;&gt; data = &#123;&quot;number_of_replicas&quot;: 2&#125;&gt;&gt;&gt; requests.put(&apos;http://localhost:9200/blog/_settings&apos;, json=data).text 读写权限参数： blocks.read_only: true【设置当前索引只允许读不允许写或者更新】 blocks.read: true【禁止对当前索引进行读操作】 blocks.write: true【禁止对当前索引进行写操作】 12345678&gt;&gt;&gt; requests.put(&apos;http://localhost:9200/blog/_settings&apos;, json=&#123;&apos;blocks.write&apos;: True&#125;).text&gt;&gt;&gt; &apos;&#123;&quot;acknowledged&quot;:true&#125;&apos;&gt;&gt;&gt; requests.put(&apos;http://localhost:9200/blog/article/1&apos;, json=&#123;&apos;test&apos;: &apos;content&apos;&#125;).text&gt;&gt;&gt; &#123;&apos;error&apos;: &#123;&apos;root_cause&apos;: [&#123;&apos;type&apos;: &apos;cluster_block_exception&apos;, &apos;reason&apos;: &apos;blocked by: [FORBIDDEN/8/index write (api)];&apos;&#125;], &apos;type&apos;: &apos;cluster_block_exception&apos;, &apos;reason&apos;: &apos;blocked by: [FORBIDDEN/8/index write (api)];&apos;&#125;, &apos;status&apos;: 403&#125; 查看索引12345678&gt;&gt;&gt; requests.get(&apos;http://localhost:9200/blog/_settings&apos;).json()&gt;&gt;&gt; &#123;&apos;blog&apos;: &#123;&apos;settings&apos;: &#123;&apos;index&apos;: &#123;&apos;number_of_shards&apos;: &apos;5&apos;, &apos;blocks&apos;: &#123;&apos;write&apos;: &apos;true&apos;&#125;, &apos;provided_name&apos;: &apos;blog&apos;, &apos;creation_date&apos;: &apos;1557197119640&apos;, &apos;number_of_replicas&apos;: &apos;1&apos;, &apos;uuid&apos;: &apos;ogHPYkvPRRm_AVazUuXXNQ&apos;, &apos;version&apos;: &#123;&apos;created&apos;: &apos;6070199&apos;&#125;&#125;&#125;&#125;&#125; 查看多个索引http://localhost:9200/blog,twitter/_settings 查看索引索引http://localhost:9200/_all/_settings 删除索引1&gt;&gt;&gt; requests.delete(&apos;http://localhost:9200/blog/&apos;) 文档管理新建文档1&gt;&gt;&gt; requests.put(&apos;http://localhost:9200/blog/article/1&apos;, json=data) 如果不指定id，elasticseach会自动生成id1&gt;&gt;&gt; requests.post(&apos;http://localhost:9200/blog/article&apos;, json=data) 获取文档1&gt;&gt;&gt; requests.get(&apos;http://localhost:9200/blog/article/1&apos;) 可以通过head命令检查文档是否存在1&gt;&gt;&gt; requests.head(&apos;http://localhost:9200/blog/article/1&apos;) 可以根据id一次获取多个文档123456789101112131415&gt;&gt;&gt; data = &#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;blog&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot; &#125;, &#123; &quot;_index&quot;: &quot;twitter&quot;, &quot;_type&quot;: &quot;tweet&quot;, &quot;_id&quot;: &quot;2&quot; &#125; ]&#125;&gt;&gt;&gt; requests.get(&apos;http://localhost:9200/_mget&apos;, json=data) 如果是同index下不同id可简写12345678910111213&gt;&gt;&gt; data = &#123; &quot;docs&quot;: [ &#123; &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot; &#125;, &#123; &quot;_type&quot;: &quot;tweet&quot;, &quot;_id&quot;: &quot;2&quot; &#125; ]&#125;&gt;&gt;&gt; requests.get(&apos;http://localhost:9200/blog/_mget&apos;, json=data) 同理，如果index和type都相同也可以简写 更新文档先索引一条文档12345PUT test/type1/1&#123; &quot;counter: &quot;1&quot;, &quot;tags&quot;: [&quot;red&quot;]&#125; 把counter字段加上4，更新命令如下：12345678910POST test/type1/1/_update&#123; &quot;script&quot;: &#123; &quot;inline&quot;: &quot;ctx._source.counter += params.count&quot;, &quot;lang&quot;: &quot;painless&quot;, &quot;params&quot;: &#123; &quot;count&quot;: 4 &#125; &#125;&#125; 命令中inline是执行的脚本，ctx是脚本语言中的一个执行对象，painless是内置的脚本语言，params是参数集合。tag字段取值为数组类型，增加一个值的命令如下：12345678910POST test/type1/1/_update&#123; &quot;script&quot;: &#123; &quot;inline&quot;: &quot;ctx._source.tag.add(params.tag)&quot;, &quot;lang&quot;: &quot;painless&quot;, &quot;params&quot;: &#123; &quot;tag&quot;: &quot;blue&quot; &#125; &#125;&#125; 删除文档1DELETE blog/article/1 如果索引时指定了路由，删除时也可以增加路由参数1DELETE blog/article/1?routing=user123 查询删除删除title中含有某term的文档12345678DELETE blog/_delete_by_query&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;hibernate&quot; &#125; &#125;&#125; 删除一个type下的所有文旦123456DELETE blog/csdn/_delete_by_query&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 批量操作版本控制不论执行多少次更新，最后保存在elasticsearch中的是最后一次更新后的文档，但是如果有两个线程同时修改一个文档，这个时候就会发生冲突。 悲观锁：认为每次去拿数据都认为别人会修改，所以要屏蔽一切有可能违反数据完整性的操作，确保同一时刻最多只有一个线程访问数据【行锁、表锁、读锁、写锁】 乐观锁：每次拿数据的时候都认为别人不会修改，假定不会发生并发访问冲突，对数据资源不会锁定，只有在数据提交时检查是否违反数据完整性。 elasticsearch使用的是乐观锁控制，文档每被修改一次，文档版本号会自增一次，_version字段确保所有的更新有序进行。演示：12345PUT /website/blog/1&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot; &#125; 查看，返回version为1：12345678910111213GET website/blog/1&#123; &quot;_index&quot; : &quot;website&quot;, &quot;_type&quot; : &quot;blog&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;My first blog entry&quot;, &quot;text&quot; : &quot;Just trying this out...&quot; &#125;&#125; 更新，此时文档版本号为212345678910111213141516171819POST website/blog/1/_update&#123; &quot;script&quot;: &quot;ctx._source.title=\&quot;Update My first blog\&quot;&quot;&#125;&#123; &quot;_index&quot; : &quot;website&quot;, &quot;_type&quot; : &quot;blog&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;result&quot; : &quot;updated&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;_seq_no&quot; : 1, &quot;_primary_term&quot; : 1&#125; 如果再请求version为1的文档会报错123456789101112131415161718192021GET website/blog/1?version=1&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[blog][1]: version conflict, current version [2] is different than the one provided [1]&quot;, &quot;index_uuid&quot;: &quot;hjsTOkFZS8OqjByPyCwJBQ&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;website&quot; &#125; ], &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[blog][1]: version conflict, current version [2] is different than the one provided [1]&quot;, &quot;index_uuid&quot;: &quot;hjsTOkFZS8OqjByPyCwJBQ&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;website&quot; &#125;, &quot;status&quot;: 409&#125; 路由机制通过哈希算法将具有相同哈希值的文档放到同一个主分片中。执行index操作时给文档设置一个routing参数，具有相同routing的文档会被分配到同一个分片上，示例如下：12345PUT website/blog/1?routing=user123&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot; &#125; 查询的时候可以根据routing值进行过滤1GET website/_search?routing=user123 文档可以有多个路由，路由值之间使用逗号隔开 映射详解【Mapping】用来定义一个文档以及其所包含的字段如何被存储和索引，可以在映射中事先定义字段的数据类型、分词器等属性。 映射分类 动态映射：elasticseach根据字段的类型自动识别 静态映射：写入数据之前对字段的属性进行手工设置123456789101112131415161718192021222324252627282930313233343536373839404142PUT booksGET books/_mapping&#123; &quot;books&quot; : &#123; &quot;mappings&quot; : &#123; &#125; &#125;&#125;PUT books/it/1&#123; &quot;id&quot;: 1, &quot;publish_date&quot;: &quot;2017-06-01&quot;, &quot;name&quot;: &quot;master&quot;&#125;GET books/_mapping&#123; &quot;books&quot; : &#123; &quot;mappings&quot; : &#123; &quot;it&quot; : &#123; &quot;properties&quot; : &#123; &quot;id&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;publish_date&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125; &#125; &#125; &#125; &#125;&#125; id、publish_date、name三个字段分别被推测为long类型、date类型和text类型。mapping可以通过dynamic设置来控制是否自动新增字段 true，默认值，自动添加字段 false，忽略新字段 strict，严格模式，发现新字段会报错 12345678910111213141516PUT books&#123; &quot;mappings&quot;: &#123; &quot;it&quot;: &#123; &quot;dynamic&quot;: &quot;strict&quot;, &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;publish_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125;&#125; 如果创建文档时存在title、publish_date以外的字段，会报错 日期检测使用date_detection设置false来关闭日期检测，如果需要增加一个date类型的字段，需要手动添加12345678PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;date_detection&quot;: false &#125; &#125;&#125; 静态映射自定义映射12345678910111213141516171819202122232425PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;user&quot;: &#123; &quot;_all&quot;: &#123;&quot;enabled&quot;: false&#125;, &quot;properties&quot;: &#123; &quot;title&quot;: &#123;&quot;type&quot;: &quot;text&quot;&#125;, &quot;name&quot;: &#123;&quot;type&quot;: &quot;text&quot;&#125;, &quot;age&quot;: &#123;&quot;type&quot;: &quot;integer&quot;&#125;, &#125; &#125;, &quot;blogpost&quot;: &#123; &quot;_all&quot;: &#123;&quot;enabled&quot;: false&#125;, &quot;properties&quot;: &#123; &quot;title&quot;: &#123;&quot;type&quot;: &quot;text&quot;&#125;, &quot;body&quot;: &#123;&quot;type&quot;: &quot;text&quot;&#125;, &quot;user_id&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;created&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; &#125; &#125; &#125; &#125;&#125; 字段类型 字符串类型：keyword string：不再支持 text：如果需要被全文检索，就使用text keyword：适用于索引结构化的字段，通常用于过滤、排序、聚合，只能通过精确值搜索到 数字类型：long、integer、short、byte、double、float、half_float、scaled_float 日期类型：date 布尔类型：boolean 二进制类型：binary 范围类型：range 数组类型：array 对象类型：object 嵌套类型：nested 地理坐标：geo_point 地理图形：geo shape IP类型：ip 范围类型：completion 令牌计数类型：token count 附件类型：attachment 抽取类型：percolator映射参数 analyzer：用于指定文本字段的分词器，对索引和查询都有效 12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125; boost：用于设置字段的权重，比如关键字出现在title字段的权重是出现在content字段中权重的两倍 1234567891011121314151617181920212223242526272829PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;boost&quot;: 2 &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &#125; &#125; &#125; &#125;&#125;# 查询时指定权重POST _search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;quick brown fox&quot;, &quot;boost&quot;: 2 &#125; &#125; &#125;&#125; copy_to：用于自定义_all字段，例如把title和content合并在一起生成一个full_content 1234567891011121314151617181920PUT myindex&#123; &quot;mapping&quot;: &#123; &quot;mytype&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;copy_to&quot;: &quot;full_content&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;copy_to&quot;: &quot;full_content&quot; &#125;, &quot;full_content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &#125; &#125; &#125; &#125;&#125; enabled: 设为false的字段，es会跳过字段内容，该字段只能从_source中获取，但是不能被搜索 properties：属性 similarity：用于指定文档评分模型【BM25，默认；classic，TF/IDF；boolean，布尔模型评分】]]></content>
      <categories>
        <category>学习记录</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka源码解析与实战（一）]]></title>
    <url>%2F2018%2F05%2F05%2FKafka%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B8%8E%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Kafka简介 Kafka是一个高度可扩展的消息系统，因其可水平扩展和高吞吐率而被广泛使用。Kafka在LinkedIn中的使用场景： 系统监控：所有主机都会忘Kafka发送系统健康信息和运行信息，负责展示运维信息和报警的系统则从Kafka订阅获取这些运维信息 传统消息队列：把Kafka作为一个分布式消息队列进行使用 分析：收集信息并通过Kafka发送懂啊hadoop集群进行分析和每日报表生成 作为其它分布式日志系统的组件 RESTful接口 审计服务 主要设计目标 以时间复杂度O(1)的方式提供消息持久化能力 高吞吐率，即使在非常廉价的商用机器上也能做到单机支持每秒100k条消息的传输 支持kafka server间的消息分区，以及分布式消费，同时保证每个分区内的消息顺序传输 支持离线数据处理和实时数据处理 支持在线水平扩展 为什么使用消息系统 解耦 冗余：在把一个消息从队列中删除之前，需要处理系统明确指出该消息已经被处理完毕 扩展性：扩展只需简单增加处理过程 灵活性和峰值处理能力 可恢复性 顺序保证 缓冲 异步通信 Kafka架构基本组成在kafka集群中，生产者将消息发送给以Topic命名的消息队列Queue中，消费者订阅发往以某个Topic命名的消息队列Queue中的消息。其中Kafka集群由多个Broker组成，Topic由若干个Partition组成，每个Partition里面的消息通过Offset来获取。 Broker：一台Kafka服务器就是一个Broker，Broker之间的地位平等 Topic：每条发送到Kafka集群的消息都属于某个主题，这个主题就称为Topic。物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存在一个或多个Broker上，但是用户只需指定消息的主题Topic即可生产或消费数据而不需要去关心数据存放何处 Partition：为了实现可扩展性，一个非常大的Topic可以被分为多个Partition，从而分布到多台Broker上。Partition中的每条消息都会被分配一个自动Id（Offset）。Kafka只保证按一个Partition中的顺序将消息发送给消费者，但是不保证单个Topic中的多个Partition之间的顺序 Offset：消息在Topic的Partition中的位置 Replica：副本 Message：消息，通信的基本单位 Producer：消息生产者 Consumer：消息消费者 Consumer Group：消费者组。每个Consumer属于一个Consumer Group；如果所有的Consumer都具有相同的Consumer Group，那么消息将会在Consumer之间进行负载均衡 Zookeeper：存放Kafka集群相关元数据的组件，Kafka通过Zookeeper管理集群配置 一个简单的消息发送流程 Producer根据指定的路由方法，将消息Push到Topic的某个Partition里 Kafka集群接收到Producer发来的消息后，将其持久化到硬盘，并保留消息指定时常【可配置】，而不关注消息是否被消费 Consumer从Kafka集群pull数据，并控制获取消息的Offset Kafka内部的通信协议每个Kafka集群中有一个Leader状态的KafkaController，Leader异常时通过Zookeeper选举出Leader。（todo）]]></content>
      <categories>
        <category>学习记录</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis哨兵模式与高可用集群]]></title>
    <url>%2F2018%2F05%2F05%2FRedis%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F%E4%B8%8E%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[前言Redis的主从复制模式下，一旦主节点由于故障不能提供服务，需要手动将从节点晋升为主节点，同时还要通知客户端更新主节点地址，这种故障处理方式从一定程度上是无法接受的。Redis 2.8 以后提供了 Redis Sentinel哨兵机制来解决这个问题。 Redis高可用概述在Redis层面，高可用的含义要宽泛一些，除了保证提供正常服务（如主从分离、快速容灾技术等），还需要考虑数据容量扩展、数据安全等等。 在Redis中，实现高可用的技术主要包括持久化、复制、哨兵和集群。 持久化：持久化是最简单的高可用方法。它的主要作用是数据备份，即将数据存储在硬盘，保证数据不会因进程退出而丢失。 复制：复制是高可用Redis的基础，哨兵和集群都是在复制基础上实现高可用的。复制主要实现了数据的多机备份以及对于读操作的负载均衡和简单的故障恢复。缺陷是故障恢复无法自动化、写操作无法负载均衡、存储能力受到单机的限制。 哨兵：在复制的基础上，哨兵实现了自动化的故障恢复。缺陷是写操作无法负载均衡，存储能力受到单机的限制。 集群：通过集群，Redis解决了写操作无法负载均衡以及存储能力受到单机限制的问题，实现了较为完善 的高可用方案。 Redis Sentinel的基本概念Redis Sentinel是Redis高可用的实现方案。Sentinel是一个管理多个Redis实例的工具，它可以实现对Redis的监控、通知、自动故障转移。基本名词说明 基本名词 逻辑结构 物理结构 Redis数据节点 主节点和从节点 主节点和从节点的进程 主节点（master） Redis主数据库 一个独立的Redis进程 从节点（slave） Redis从数据库 一个独立的Redis进程 Sentinel节点 监控Redis数据节点 一个独立的Sentinel进程 Sentinel节点集合 若干Sentinel节点的抽象组合 若干Sentinel节点进程 Redis Sentinel Redis高可用实现方案 Sentinel节点集合和Redis数据节点进程 应用客户端 泛指一个或多个客户端 一个或多个客户端进程或线程 Redis主从复制的问题 一旦主节点宕机，从节点晋升成主节点，同时需要修改应用方的主节点地址，还需要命令所有从节点去复制新的主节点，整个过程需要人工干预； 主节点的写能力受到单机的限制； 主节点的存储能力受到单机的限制； 原生复制的弊端在早期的版本中也会比较突出，比如：Redis复制中断后，从节点会发起psync。此时如果同步不成功，则会进行全量同步，主库执行全量备份的同时，可能会造成毫秒或秒级的卡顿。 Redis Sentinel深入探究Sentinel的主要功能包括主节点存活检测、主从运行情况检测、自动故障转移（failover）、主从切换。Redis的Sentinel系统可以用来管理多个Redis服务器，该系统可以执行以下四个任务： 监控：Sentinel会不断的检查主服务器和从服务器是否正常运行； 通知：当被监控的某个Redis服务器出现问题，Sentinel通过API脚本向管理员或者其他的应用程序发送通知。 自动故障转移：当主节点不能正常工作时，Sentinel会开始一次自动的故障转移操作，它会将与失效主节点是主从关系的其中一个从节点升级为新的主节点，并且将其他的从节点指向新的主节点。 配置提供者：在Redis Sentinel模式下，客户端应用在初始化时连接的是Sentinel节点集合，从中获取主节点的信息。 主观下线和客观下线默认情况下，每个Sentinel节点会以每秒一次的频率对Redis节点和其它的Sentinel节点发送PING命令，并通过节点的回复来判断节点是否在线。 主观下线：主观下线适用于所有主节点和从节点。如果在down-after-milliseconds毫秒内，Sentinel没有收到目标节点的有效回复，则会判定该节点为主观下线。 客观下线：客观下线只适用于主节点。如果主节点出现故障，Sentinel节点会通过sentinel is-master-down-by-addr命令，向其它Sentinel节点询问对该节点的状态判断。如果超过个数的节点判定主节点不可达，则该Sentinel节点会判断主节点为客观下线。 Sentinel的通信命令Sentinel节点连接一个Redis实例的时候，会创建cmd和pub/sub两个连接。Sentinel通过cmd连接给Redis发送命令，通过pub/sub连接到Redis实例上的其他Sentinel实例。Sentinel与Redis主节点和从节点交互的命令，主要包括： 命令 作用 PING Sentinel 向 Redis 节点发送 PING 命令，检查节点的状态 INFO Sentinel 向 Redis 节点发送 INFO 命令，获取它的从节点信息 PUBLISH Sentinel 向其监控的 Redis 节点 __sentinel__:hello 这个 channel 发布自己的信息及主节点相关的配置 SUBSCRIBE Sentinel 通过订阅 Redis 主节点和从节点的 __sentinel__:hello这个 channel ，获取正在监控相同服务的其它 Sentinel节点 Sentinel与Sentinel交互的命令，主要包括： 命令 作用 PING Sentinel 向其它 Sentinel 节点发送 PING 命令，检查节点的状态 SENTINEL:is-master-down-by-addr 和其它 Sentinel 协商主节点的状态，如果主节点处于 SDOWN 状态，则投票自动选出新的节点 Redis Sentinel的工作原理 每个Sentinel以每秒钟一次的频率，向它所知的主服务器、从服务器以及其他Sentinel实例发送一个PING命令。 如果一个实例距离最后一次有效回复PING命令的时间超过down-after-milliseconds所指定的值，那么这个实例会被Sentinel标记为主观下线。 如果一个主服务器被标记为主观下线，那么正在监视这个主服务器的所有Sentinel节点，要以每秒一次的频率确认主服务器的确进入了主观下线状态。 如果一个主服务器被标记为主观下线，并且有足够数量的Sentinel（至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断，那么这个主服务器被标记为客观下线。 在一般情况下， 每个Sentinel会以每10秒一次的频率，向它已知的所有主服务器和从服务器发送INFO命令。当一个主服务器被Sentinel标记为客观下线时，Sentinel向下线主服务器的所有从服务器发送INFO命令的频率，会从10秒一次改为每秒一次。 Sentinel和其他Sentinel协商主节点的状态，如果主节点处于SDOWN状态，则投票自动选出新的主节点。将剩余的从节点指向新的主节点进行数据复制。 当没有足够数量的Sentinel同意主服务器下线时，主服务器的客观下线状态就会被移除。当主服务器重新向Sentinel的PING命令返回有效回复时，主服务器的主观下线状态就会被移除。 Redis Sentinel搭建Redis Sentinel的部署须知 一个稳健的Redis Sentinel集群，应该使用至少三个Sentinel实例，并且保证讲这些实例放到不同的机器上，甚至不同的物理区域。 Sentinel无法保证强一致性 常见的客户端应用库都支持Sentinel Sentinel需要通过不断的测试和观察，才能保证高可用 Redis Sentinel的配置文件 新建sentinel.conf文件 sentinel monitor [被监控数据库名字【自己起】] ip port 1 【1表示投票票数达到数量】 启动哨兵：redis-sentinel 配置文件 可监控多个master TODO]]></content>
      <categories>
        <category>学习记录</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>主从复制</tag>
        <tag>哨兵机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis为什么是单线程]]></title>
    <url>%2F2018%2F05%2F04%2FRedis%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%95%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Redis采用的是基于内存单进程单线程模型的KV数据库，由C语言编写。 它可以用作：数据库、缓存和消息中间件。 简介Redis支持多种数据结构，如字符串（String），散列（Hash），列表（List），集合（Set），有序集合（Zset），以及Bitmaps、Hyperloglogs和地理空间（Geospatial）。 内置了复制（Replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（Transactions） 和不同级别的磁盘持久化（Persistence），并通过Redis哨兵（Sentinel）和自动分区（Cluster）提供高可用性（High Availability）。 Redis也提供了持久化的选项，这些选项可以让用户将自己的数据保存到磁盘上面进行存储。根据实际情况，可以每隔一定时间将数据集导出到磁盘（快照），或者追加到命令日志中（AOF只追加文件） Redis将数据储存在内存里面，读写数据的时候都不会受到硬盘I/O速度的限制，所以速度极快。 为什么快 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap； 数据结构简单，对数据操作也简单； 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或多线程切换而消耗cpu，不用考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 使用多路I/O复用模型，非阻塞IO； 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 多路I/O复用模型多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。 这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗）。 和Memcached不同，Redis并没有直接使用Libevent，而是自己完成了一个非常轻量级的对select、epoll、evport、kqueue这些通用的接口的实现。在不同的系统调用选用适合的接口，linux下默认是epoll。因为Libevent比较重更通用代码量也就很庞大，拥有很多Redis用不上的功能，Redis为了追求“轻巧”并且去除依赖，就选择自己去封装了一套。 为什么采用单线程官方FAQ表示，因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）。 这里我们一直在强调的单线程，只是在处理我们的网络请求的时候只有一个线程来处理，一个正式的Redis Server运行的时候肯定是不止一个线程的，例如Redis进行持久化的时候会以子进程或者子线程的方式执行。 从Redis 4.0版本开始会支持多线程的方式，但是，只是在某一些操作上进行多线程的操作！]]></content>
      <categories>
        <category>学习记录</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>单线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go基础学习(二)]]></title>
    <url>%2F2018%2F05%2F04%2FGo%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[数组【array】 数组定义【定义后长度不可变】12symbol := [...]string&#123;USD: &quot;$&quot;, EUR: &quot;€&quot;, GBP: &quot;￡&quot;, RMB: &quot;￥&quot;&#125;fmt.Println(RMB, symbol[RMB]) 指针数组12345678910111213var array3 [5]*intfor i := range array3 &#123; array3[i] = new(int)&#125;*array3[0] = 1*array3[1] = 2*array3[2] = 3*array3[3] = 4*array3[4] = 5array4 := array3for _, v := range array4 &#123; fmt.Println(*v)&#125; 切片【可变数组】切片在底层维护一个可以动态扩展的数组，切片这一数据结构包含三个元素，指针、长度、容量【所以切片是引用类型】12345678source := make([]string, 3, 5) // 访问长度3个元素，底层数组拥有5个元素，不允许创建容量小于长度的切片source = []string&#123;&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;black&quot;&#125;slice := source[2:3:5] // 1个元素，容量为5-2=3for _, v := range slice &#123; fmt.Println(v)&#125;fmt.Println(len(slice))fmt.Println(cap(slice)) 切片、字符串互相转换123456source := make([]string, 10)source = []string&#123;&quot;123&quot;, &quot;456&quot;, &quot;789&quot;&#125;a := strings.Join(source, &quot;,&quot;) // 切片转字符串m := strings.Split(a, &quot;,&quot;)fmt.Println(a)fmt.Println(m) 中间插入元素1234567a := []int&#123;1, 2, 3, 4, 5&#125;a = append(a, 0)fmt.Println(a)copy(a[3:], a[2:]) // 将[2:]的元素拷贝到[3:]中，最后一个元素丢弃fmt.Println(a)a[2] = 10 // 修改元素为新元素fmt.Print(a) 原地删除123456789// 原地删除，公用底层数组s := []string&#123;&quot;a&quot;, &quot;b&quot;, &quot; &quot;, &quot;d&quot;, &quot;e&quot;, &quot; &quot;, &quot;f&quot;&#125;b := s[:0]for _, x := range s &#123; if x != &quot; &quot; &#123; b = append(b, x) // append函数用于追加元素 &#125;&#125;fmt.Println(b) slice作为参数1234567// more本质上就是slice类型func Sum(a int, more ...int) int &#123; for _, v := range more &#123; a += v &#125; return a&#125; Map基本操作12345678910dict := make(map[string]int)dict[&quot;abc&quot;] = 123dict[&quot;efg&quot;] = 456fmt.Println(dict[&quot;abc&quot;])_, exist := dict[&quot;abcd&quot;]fmt.Println(exist) // 判断是否存在，如果不赋值，exist就是false，赋值哪怕是零值，也为truedelete(dict, &quot;abcd&quot;) // 即使key不存在也不会报错for index, value := range dict &#123; // map遍历是无序的 fmt.Println(index, value)&#125; map类型也是引用类型，所以如果作为参数传到函数中修改，会改变变量本身 structstruct不是引用类型，改变自身需要传指针继承12345678type Point struct &#123; X, Y int&#125;type Circle struct &#123; Point Radius int&#125; interfaceinterface是一组method的组合，可以通过interface来定义对象的一组行为。如果某个对象实现了某个接口的所有方法，则此对象就实现了此接口。如果我们定义了一个interface的变量，那么这个变量里面可以存实现这个interface的任意类型的对象。类型断言，判断一个interface是否是该类型123456789101112131415161718192021222324252627282930313233package mainimport &quot;fmt&quot;type Element interface&#123;&#125;type List []Elementtype People struct &#123; Name string&#125;func main() &#123; list := make(List, 3) list[0] = 1 list[1] = &quot;hello&quot; list[2] = People&#123;&quot;test&quot;&#125; for _, value := range list &#123; n, ok := value.(int) if ok &#123; fmt.Println(n) continue &#125; s, ok := value.(string) if ok &#123; fmt.Println(s[:3]) continue &#125; p, ok := value.(People) if ok &#123; fmt.Println(p.Name) continue &#125; &#125;&#125; 使用switch来简化代码【element.(type)】不能在switch外的任何逻辑里面使用1234567891011121314151617181920212223242526272829package mainimport &quot;fmt&quot;type Element interface&#123;&#125;type List []Elementtype People struct &#123; Name string&#125;func main() &#123; list := make(List, 4) list[0] = 1 list[1] = &quot;hello&quot; list[2] = People&#123;&quot;test&quot;&#125; list[3] = 2.3 for _, value := range list &#123; switch t := value.(type) &#123; case int: fmt.Println(t + 1) case string: fmt.Println(t[:3]) case People: fmt.Println(t.Name) default: fmt.Println(&quot;类型异常&quot;) &#125; &#125;&#125; interface也可以向struct一样内嵌]]></content>
      <categories>
        <category>学习记录</category>
        <category>golang</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>数组</tag>
        <tag>切片</tag>
        <tag>结构体</tag>
        <tag>接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go基础学习(一)]]></title>
    <url>%2F2018%2F05%2F03%2FGo%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[变量 从计算系统实现的角度来看，变量是一段或多段用来存储数据的内存； 类型决定了变量内存的长度和存储格式； 建议以组方式整理多行变量定义： 1234var ( x, y int a, s = 100, &quot;abc&quot;) 在进行多变量赋值操作时，首先计算出所有的右值，然后在依次完成赋值操作； 命名建议： 使用驼峰拼写格式 局部变量优先使用短名 不要使用保留关键字 不建议使用与预定义常量、类型、内置函数相同的名字 专有名词通常会全部大写 首字母大小写决定了其作用域，首字母大写的可被外包引用，小写则仅能在包内使用； 常量 常量值必须是编译期可确定的字符、字符串、数字或者布尔值； 不曾使用的常量不会引发编译错误； 在常量组中如不指定类型和初始化值，则与上一行非空常量右值（表达式文本）相同： 123456const ( x uint16 = 120 y // 与x类型、右值相同 s = &quot;abc&quot; z // 与s类型、右值相同) 引用类型 所谓引用类型（reference type）特指slice、map、channel这三种预定义类型； 相比数字、数组等类型，引用类型拥有更复杂的存储结构。除分配内存外，它们还需初始化一系列属性，诸如指针、长度，甚至包括哈希分布、数据队列等； 内置函数new按指定类型长度分配零值内存，返回指针，并不关心类型内存构造和初始化方式。而引用类型则必须使用make函数创建，编译器会将make转化为目标类型专用的创建函数或指令，以确保完成全部内存分配和相关属性初始化； 基础数据类型string类型123456789101112131415s := &quot;hello world&quot;fmt.Println(len(s)) // 输出字符串长度fmt.Println(s[0], s[7]) // 104 111【单个元素为字节值，ascii码】fmt.Println(s[0:1]) // h【子串则返回对应的字符】fmt.Println(s[:5]) // 返回字符串==========s := &quot;hello世界&quot; // 字符串里带中文fmt.Println(len(s)) // 长度为11，一个中文占三个字节a := []rune(s) // Unicode码点对应rune整数类型【rune是int32等价类型】fmt.Println(len(a)) // 长度为7for v := range a &#123; // 依次输出 h、e、l、l、o、世、界 fmt.Printf(&quot;%c\n&quot;, a[v])&#125; 整型123456789101112131415161718192021222324var u uint8 = 255/* u 11111111 u + 1 00000000 高位抛弃 u * u 二进制数乘法，高位抛弃*/fmt.Println(u, u+1, u*u)fmt.Printf(&quot;%08b, %08b, %08b\n&quot;, u, u+1, u*u)var i int8 = 127/* i 01111111 【正数的反码、补码就是原码本身】 i + 1 10000000 反码【1111111】原码【10000000】= 128，再加负号 i + 1 + 1 10000001 反码【0000000】原码【1111111】= 127，再加负号*/fmt.Println(i, i+1, i+1+1)fmt.Printf(&quot;%08b, %08b, %08b\n&quot;, i, i+1, i+1+1)var x uint8 = 1&lt;&lt;1 | 1&lt;&lt;5 // 00000010 | 00100000 = 00100010var y uint8 = 1&lt;&lt;1 | 1&lt;&lt;2 // 00000010 | 00000100 = 00000110fmt.Printf(&quot;%d:%08[1]b, %d:%08[2]b\n&quot;, x, y)fmt.Printf(&quot;%d:%08[1]b\n&quot;, x&amp;y) // 00100010 &amp; 00000110 = 00000010fmt.Printf(&quot;%d:%08[1]b\n&quot;, x^y) // 00100010 ^ 00000110 = 00100100 异或fmt.Printf(&quot;%d:%08[1]b\n&quot;, ^y) // ^ 00000110 = 11111001 取反]]></content>
      <categories>
        <category>学习记录</category>
        <category>golang</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo功能探索]]></title>
    <url>%2F2018%2F04%2F03%2Fhexo%E5%8A%9F%E8%83%BD%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[使用hexo搭建个人博客，记录相关功能的实现方法。 文章分类实现对应md文件的head部分增加相关设置123456title: titledate: 2019-05-02 15:08:57tags: categories: - 一级分类 - 二级分类 其它的，还有tag、comments、toc等参数可以设置 导航栏设置修改对应主题下的配置文件_config.yml。菜单栏设置12345678910111213menu: home: / || home categories: /categories/ || th tags: /tags/ || tags # schedule: /schedule/ || calendar # sitemap: /sitemap.xml || sitemap # commonweal: /404/ || heartbeat archives: /archives/ || archive about: /about/ || usermenu_settings: icons: true badges: true 设置完对应的菜单标签之后，还需要创建对应的文件夹123hexo new page &apos;tags&apos;hexo new page &apos;categories&apos;hexo new page &apos;about&apos; 然后在对应index.md文件中设置type属性123title: 文章分类date: 2019-05-02 15:34:18type: categories 布局设置12345# Schemes# scheme: Musescheme: Mist# scheme: Pisces# scheme: Gemini 显示摘要加入&lt;!-- more --&gt;标签，该标签之上的内容为文章摘要，首页只显示文章摘要。 插入图片使用markdown语法插入图片，前提是执行npm install hexo-asset-image安装相关插件，否则生成的html文件中的图片路径会不准确。 简单加密由于页面是静态html，所以在html中加入js实现简单加密。找到themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig文件，加入代码12345678910&lt;script&gt; (function()&#123; if(&apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; alert(&apos;密码错误！&apos;); history.back(); &#125; &#125; &#125;)();&lt;/script&gt; page.password就是md文件中的变量，md文件中设置password参数就能实现简单的权限控制。 搜索功能：安装插件：1npm install hexo-generator-searchdb --save 在根目录下的/theme/next/_config.yml文件中添加配置：12345search: path: search.xml field: post format: html limit: 10000 在根目录下的/theme/next/_config.yml文件修改配置12local_search: enable: true 音乐 视频 字数和阅读时长安装插件1npm install hexo-symbols-count-time 博客根目录配置12345symbols_count_time: symbols: true # 文章字数 time: true # 阅读时长 total_symbols: true # 所有文章总字数 total_time: true # 所有文章阅读中时长 next主题配置123456symbols_count_time: separated_meta: true # 是否换行显示 字数统计 及 阅读时长 item_text_post: true # 文章 字数统计 阅读时长 使用图标 还是 文本表示 item_text_total: false # 博客底部统计 字数统计 阅读时长 使用图标 还是 文本表示 awl: 4 wpm: 275]]></content>
      <categories>
        <category>学习记录</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F04%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>学习记录</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
